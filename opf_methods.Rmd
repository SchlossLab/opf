---
title: "OPF methods"
author: "Kathryn Iverson"
date: "January 15, 2015"
output: html_document
---

## Overview

The sequence reads from the 20 metagenomes generated by Koren and colleageus (Koren et al., 2012) in their study of the effects of pregnancy on the mother’s microbiome were obtained from MG-RAST.  The reads were pooled and assembled using an iterative assembly. Reads were first assembled with Velvet using a k-mer length of 71 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes) and unused reads were retained. The unused reads were then assembled with k-mer lengths 61, 51 and 41 with the same parameters. The contigs from each of these assemblies were merged with minimus2 (part of AMOS; Sommer et al., 2007) using default parameters.  An average of 54.43% (range 24.23% to 71.78%) of all reads across 20 metagenomes mapped back to the assembled contigs.

The sequence reads from the 18 metagenomes generated by Turnbaugh and colleagues (2009) in their study of the microbiome of lean and obese individuals were obtained from MG-RAST. The reads from all metagenomes were pooled and assembled with Velvet (Zerbino and Birney, 2008) using k 51 (parameters -exp_cov auto -min_contig_lgth 100). The reads from this study were treated as ‘long’ because they were sequenced with 454 pyrosequencing (mean length 216bp, range 9bp to 665bp) whereas the Illumina reads from the pregnancy and mouse studies were treated as ‘short’ (mean length 99bp and 98bp, respectively).  An average of 22.13% (range 4.72% to 35.90%) of all reads across 18 metagenomes mapped back to the assembled contigs.


## Download data

### Downloading raw reads from MG-RAST

First, create a directory to store the data from both the pregnancy and twin studies.

```
mkdir pregnancy_data
mkdir twin_data
```

MG-RAST has an api for downloading data. They provide python scripts to access this api as well as a structured URL. First, download their api from github.

```
git clone https://github.com/MG-RAST/MG-RAST-Tools.git
```
There is a script provided to set up the shell environment and can be run as per their instructions simply by running `.\set_env.sh` inside the tools directory. This script didn't work on all my machines but all it's doing is setting the PYTHONPATH variable and this is easy to do manually. Simply add the absolute path to the MG-RAST-Tools/tools/lib/ directory.

```
cd MG-RAST-Tools
MG_DIR=`pwd`
PYTHONPATH=$PYTHONPATH:$MG_DIR/tools/lib/
```

To download the data from the pregnancy study, move into the data directory.

```
cd ../pregnancy_data
```

Next, use the MG-RAST tools to download all the raw reads for this dataset. First, get a list of the metagenomes.

```
../MG-RAST-Tools/tools/bin/mg-download.py --project mgp265 --list | sed 's/\t.*$//g' | sed '1d' | uniq > metagenomes.list
```

To download the actual files, use curl. This can be done in parallel if gnu parallel is installed. Otherwise a for-loop will also work.

```
cat metagenomes.list | parallel "curl http://api.metagenomics.anl.gov//download/{}?file=050.1 | tee >(md5sum > {}.md5sum) | gzip > {}.fna.gz"
```

Optional for-loop:
```
for MG in metagenomes.list; do
	curl http://api.metagenomics.anl.gov//download/${MG}?file=050.1 | tee >(md5sum > {}.md5sum)  | gzip > ${MG}.fna.gz
done
```

This will zip the raw fasta files and save the md5sums to validate the download.

File list:
```
mgm4474351.3    mgm4474351.3.050.upload.fna	050.1	9ae1567e29c92695d4dba87368d48c91	2725914863
mgm4467109.3	mgm4467109.3.050.upload.fna	050.1	585207b8d1ed08054f64d83e2ea2c4b5	11994956359
mgm4466188.3	mgm4466188.3.050.upload.fna	050.1	06d134cc37353ae4ad61a114f864a680	16364852358
mgm4474355.3	mgm4474355.3.050.upload.fna	050.1	880b848b7b43042196121024ab84a226	2897033362
mgm4474361.3	mgm4474361.3.050.upload.fna	050.1	629cd1f2459d20ea17c2520191eb70d3	2216895877
mgm4466296.3	mgm4466296.3.050.upload.fna	050.1	835d93b4898cedba6fda521558f36b8c	17698558630
mgm4474805.3	mgm4474805.3.050.upload.fna	050.1	c2b677e9fdf510994a938ad40caeaaa1	2863660233
mgm4466602.3	mgm4466602.3.050.upload.fna	050.1	374d849ad2cf90d6cc0dd73f4406be41	17061324126
mgm4474369.3	mgm4474369.3.050.upload.fna	050.1	b6076f12d6e14386dcd907c7d743d06e	2856542403
mgm4474352.3	mgm4474352.3.050.upload.fna	050.1	b96c0fe18613e766ab05bd86aea11ae3	1846447145
mgm4466257.3	mgm4466257.3.050.upload.fna	050.1	b4106a991a7ed7b4ea2a74b5b4c7183b	12188538201
mgm4474360.3	mgm4474360.3.050.upload.fna	050.1	cf599631d45f0618d43232a57f616d7c	2845729091
mgm4466194.3	mgm4466194.3.050.upload.fna	050.1	d51674481f637a48bf5a430fd6c2b507	12063818770
mgm4474358.3	mgm4474358.3.050.upload.fna	050.1	01d8bcc929984938ffc595a04e442122	1538258029
mgm4474357.3	mgm4474357.3.050.upload.fna	050.1	a6202d3e5dda9539970260c3f516af29	2290285473
mgm4466148.3	mgm4466148.3.050.upload.fna	050.1	7dfdd296b29c086d03ae777cb37549a3	16747713282
mgm4474359.3	mgm4474359.3.050.upload.fna	050.1	a3c700b971b1e4a0aa1b7e712c10f21d	2395798868
mgm4469858.3	mgm4469858.3.050.upload.fna	050.1	391d1538dec85fec2f72d1d7f7e3c663	8594938232
mgm4466149.3	mgm4466149.3.050.upload.fna	050.1	8f7a42a3a8c42b86e0f1a35fbb8113bc	7808085470
mgm4466163.3	mgm4466163.3.050.upload.fna	050.1	19f8dec05c5bb33f1810e06de68f98a7	14472271099
```


Download the data from the twin study the same way. The only difference is this is MG-RAST project 10 (mgp10)

```
cd ../twin_data
../MG-RAST-Tools/tools/bin/mg-download.py --project mgp10 --list | sed 's/\t.*$//g' | sed '1d' | uniq > metagenomes.list
cat metagenomes.list | parallel "curl http://api.metagenomics.anl.gov//download/{}?file=050.1 | tee >(md5sum > {}.md5sum) | gzip > {}.fna.gz"
```

## Digital normalization

The reads were normalized by median using khmer with k-mer length of 20, count of 20 and hash size of 1e10 (Brown et al., 2012). This excluded from assembly any read that had a median k-mer of length 20 that had previously been encountered at least 20 times. This data reduction step removed redundant reads unlikely to add any more information to the assembly. The excluded reads were saved for downstream analysis. The remaining reads were filtered by abundance removing any low abundance (<5) and unique k-mers that are unlikely to assemble into larger contigs.

```
#files
SEQS=$1
#paths
KHMER_SCRIPTS="/home/kiverson/khmer/scripts" #path to khmer/scripts

#parameters
K=20
C=20
X_PRAM='1e10'

python $KHMER_SCRIPTS/normalize-by-median.py -k $K -C $C -x $X_PRAM --savetable $SEQS.kh $SEQS
python $KHMER_SCRIPTS/filter-abund.py -V $SEQS.kh $SEQS.keep
rm $SEQS.kh
```

## Assembly

The reads were pooled and assembled using an iterative assembly. Reads were first assembled with Velvet using a k-mer length of 71 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes) and unused reads were retained. The unused reads were then assembled with k-mer lengths 61, 51 and 41 with the same parameters. The contigs from each of these assemblies were merged with minimus2 (part of AMOS; Sommer et al., 2007) using default parameters.  An average of 54.43% (range 24.23% to 71.78%) of all reads across 20 metagenomes mapped back to the assembled contigs.

The filtered reads were pooled, `cat *.keep.abundfilt > all.abundfilt`, and assembled using an iterative assembly.

```
mkdir k.49;
velveth k.49 49 -fasta -short all.abundfilt;
velvetg k.49 -exp_cov auto -unused_reads yes;
rm k.49/Sequences;
rm k.49/*Graph*

for k in {47..21..2};
do
	mkdir k.$k;
	velveth k.$k $k -fasta -short all.abundfilt;
	velvetg k.$k -exp_cov auto -unused_reads yes;
	rm k.$k/Sequences;
	rm k.$k/*Graph*
done;
```

If using job submission software like pbs it would be a good idea so submit each velvet call as a dependent job.

Combine all contigs, `cat k.*/Contigs.fa > all.contigs.fa`.

## Map reads to contigs

To get coverate map reads to the assembled contigs using bowtie.

```
bowtie-build
bowtie
```

## Gene prediction

Genes were predicted from assembled contigs with MetaGeneAnnotator (Noguchi et al., 2008). The predicted genes were separated into two groups: having both a start and stop codon (full length) or fragments with a length greater than 300 nucleotides, including the full length group.

```
mga
```


HMP
de-replicate at 99% BEFORE blast to reduce data size
usearch for all v all
ALL READS mapping for counts


Gene prediction
Genes were predicted from assembled contigs with MetaGeneAnnotator (Noguchi et al., 2008). The predicted genes were separated into two groups: having both a start and stop codon (full length) or fragments with a length greater than 300 nucleotides, including the full length group.

Functional and taxonomic classification
Predicted gene fragments were searched against the eggNOG database (Muller et al., 2010) by BLASTp. Significant hits that had e-values > 10^-5 were grouped into eggNOG categories. Significant hits were defined as any hit that had an e-value <= 10*e-value of the top hit (Qin et al., 2010).

OPF creation and data analysis
Abundance was calculated by number of reads mapping onto predicted genes and fragments,  and normalized for gene length. Mapping was done using bowtie (Langmead et al., 2009). OPFs were created using the mgcluster command in mothur (Schloss et al. 2009). For all analyses the datasets were subsampled to the lowest number of sequences per sample. A Principal component analysis  (PCoA) plot was generated in mothur using the average Yue-Clayton measure of dissimilarity distances (θYC; REF). Plots were generated with R (R Development Core Team, 2012). Similarly a distance matrix was generated from abundance of KEGG categories.
The OPF abundances were transformed into binary data and OPF occurrences within and between groups of samples inside each dataset were analyzed with R.  For the mice data, we combined the 16S and OPF data using correlation analysis and these results are summarized in a heatmap showing the 20 OTUs that had significantly different abundance during the early and late periods and the OPFs correlating with these.








=======================

For the mice study the primer sequences were removed from raw reads using SeqPrep (available at https://github.com/jstjohn/seqPrep).

```
seqPrep
```



```
khmer
```

Obesity study dataset.  

The sequence reads from the 18 metagenomes generated by Turnbaugh and colleagues (2009) in their study of the microbiome of lean and obese individuals were obtained from MG-RAST.

```
wget
```

The reads from all metagenomes were pooled and assembled with Velvet (Zerbino and Birney, 2008) using k 51 (parameters -exp_cov auto -min_contig_lgth 100). The reads from this study were treated as ‘long’ because they were sequenced with 454 pyrosequencing (mean length 216bp, range 9bp to 665bp) whereas the Illumina reads from the pregnancy and mouse studies were treated as ‘short’ (mean length 99bp and 98bp, respectively).  An average of 22.13% (range 4.72% to 35.90%) of all reads across 18 metagenomes mapped back to the assembled contigs.

Pregnancy study dataset.  

The sequence reads from the 20 metagenomes generated by Koren and colleageus (Koren et al., 2012) in their study of the effects of pregnancy on the mother’s microbiome were obtained from MG-RAST.  The reads were pooled and assembled using an iterative assembly. Reads were first assembled with Velvet using a k-mer length of 71 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes) and unused reads were retained. The unused reads were then assembled with k-mer lengths 61, 51 and 41 with the same parameters. The contigs from each of these assemblies were merged with minimus2 (part of AMOS; Sommer et al., 2007) using default parameters.  An average of 54.43% (range 24.23% to 71.78%) of all reads across 20 metagenomes mapped back to the assembled contigs.

Mouse gut microbiome stability dataset.  

Using the Illumina HiSeq and standard methods (http://hmpdacc.org/doc/HMP_MOP_Version12_0_072910.pdf), we generated 48 metagenomes from samples for which we previously obtained 16S rRNA gene sequence data (Schloss et al., 2012).  The sequencing was performed at the Baylor College of Medicine (Houston, TX).  All reads from the mice study were pooled into two groups corresponding to early and late samples. After normalization 15% of the late reads (false positive rate of 0.000) and 17% of the early (false positive rate of 0.258) were retained for assembly. The unused reads were set aside and used in downstream analysis.  The retained reads were assembled twice with Velvet using k 31 and k 35 (parameters -exp_cov auto -cov_cutoff 0 -scaffolding no). Both assemblies were merged with minimus2 using default parameters.  An average of 84% (range XX.XX% to XX.XX%) of all reads across 48 metagenomes mapped back to the assembled contigs.

Gene prediction and classification.  

Genes were predicted from assembled contigs with MetaGeneAnnotator (Noguchi et al., 2008). The predicted genes were filtered to only include predicted genes and fragments with a length greater than 300 nucleotides.  Predicted genes and fragments were translated and searched against the KEGG database (Kanehisa et al., 2012) by BLASTp. Significant hits that had e-values less than 10^-5 were grouped into KEGG modules.  Significant hits were defined as any hit that had an e-value less than or equal to 10 times the e-value of the top hit (Qin et al., 2010).

OPF creation and data analysis.  

Abundance was calculated by number of reads mapping onto predicted gene fragments and normalized for gene length. Mapping was done using Bowtie (Langmead et al., 2009) for the mice and pregnancy reads and BWA-SW (Li and Durbin, 2010) for the obesity reads.  OPFs were created from the predicted genes and fragments using the mgcluster command in mothur (Schloss et al., 2009).  To minimize the effects of uneven sampling, within a dataset each sample was rarefied to the lowest number of sequences per sample.  Principal component analysis (PCoA) plots were generated in mothur using the average Yue-Clayton measure of dissimilarity distances (θYC; Yue and Clayton, 2005) or weighted or unweighted Unifrac distances (Lozupone and Knight, 2005).  Statistical testing was done within the R environment (R Development Core Team, 2012) and mothur.




Data processing
Quality assurance, normalization
SeqPrep (available at https://github.com/jstjohn/seqPrep) was used to remove primer sequences from reads.  Reads were normalized by median using khmer with k-mer length of 20, count of 20, hash size 1e9 and 4 hashes (parameters -k 20 -C 20 -x 1e9 -N 4) (Brown et al. 2012). This excluded from assembly any read that had a median k-mer of length 20 that had previously been encountered at least 20 times. This data reduction step removed redundant reads unlikely to add any more information to the assembly. The excluded reads were saved for downstream analysis. The remaining reads were filtered by abundance using filter-abund.py. This removed any low abundance and unique k-mers that are unlikely to assemble into larger contigs.

Assembly
To demonstrate robustness to assembly methods the normalized reads were assembled using two assembly methods.

Mice and twin
Reads were assembled twice with velvet (Zerbino and Birney, 2008) using k 31 and k 35 (parameters -exp_cov auto -cov_cutoff 0 -scaffolding no). Both assemblies were merged with minimus2,part of AMOS (Sommer et al., 2007), using default parameters.

Pregnancy
Reads from the pregnancy data were assembled using an iterative assembly. Reads were assembled with velvet using k 71 and unused reads were retained. These unused reads were then assembled with k 61, k 51 and k 41 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes). The contigs from each of these assemblies were merged with minimus2 using default parameters.
