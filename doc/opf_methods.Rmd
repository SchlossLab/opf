---
title: "OPF methods"
author: "Kathryn Iverson"
date: "January 15, 2015"
output: html_document
---

# Overview

The sequence reads from the 20 metagenomes generated by Koren and colleageus (Koren et al., 2012) in their study of the effects of pregnancy on the mother’s microbiome were obtained from MG-RAST.  The reads were pooled and assembled using an iterative assembly. Reads were first assembled with Velvet using a k-mer length of 71 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes) and unused reads were retained. The unused reads were then assembled with k-mer lengths 61, 51 and 41 with the same parameters. The contigs from each of these assemblies were merged with minimus2 (part of AMOS; Sommer et al., 2007) using default parameters.  An average of 54.43% (range 24.23% to 71.78%) of all reads across 20 metagenomes mapped back to the assembled contigs.

The sequence reads from the 18 metagenomes generated by Turnbaugh and colleagues (2009) in their study of the microbiome of lean and obese individuals were obtained from MG-RAST. The reads from all metagenomes were pooled and assembled with Velvet (Zerbino and Birney, 2008) using k 51 (parameters -exp_cov auto -min_contig_lgth 100). The reads from this study were treated as ‘long’ because they were sequenced with 454 pyrosequencing (mean length 216bp, range 9bp to 665bp) whereas the Illumina reads from the pregnancy and mouse studies were treated as ‘short’ (mean length 99bp and 98bp, respectively).  An average of 22.13% (range 4.72% to 35.90%) of all reads across 18 metagenomes mapped back to the assembled contigs.


## Pregnancy dataset

### Downloading raw reads from MG-RAST

First, create a directory to store the data from both the pregnancy and twin studies.

```
mkdir pregnancy_data
mkdir twin_data
```

MG-RAST has an api for downloading data. They provide python scripts to access this api as well as a structured URL. First, download their api from github.

```
git clone https://github.com/MG-RAST/MG-RAST-Tools.git
```
There is a script provided to set up the shell environment and can be run as per their instructions simply by running `.\set_env.sh` inside the tools directory. This script didn't work on all my machines but all it's doing is setting the PYTHONPATH variable and this is easy to do manually. Simply add the absolute path to the MG-RAST-Tools/tools/lib/ directory.

```
cd MG-RAST-Tools
MG_DIR=`pwd`
PYTHONPATH=$PYTHONPATH:$MG_DIR/tools/lib/
```

To download the data from the pregnancy study, move into the data directory.

```
cd ../pregnancy_data
```

Next, use the MG-RAST tools to download all the raw reads for this dataset. First, get a list of the metagenomes.

```
../MG-RAST-Tools/tools/bin/mg-download.py --project mgp265 --list | sed 's/\t.*$//g' | sed '1d' | uniq > metagenomes.list
```

To download the actual files, use curl. This can be done in parallel if gnu parallel is installed. Otherwise a for-loop will also work.

```
cat metagenomes.list | parallel "curl http://api.metagenomics.anl.gov//download/{}?file=050.1 | tee >(md5sum > {}.md5sum) | gzip > {}.fna.gz"
```

Optional for-loop:
```
for MG in metagenomes.list; do
	curl http://api.metagenomics.anl.gov//download/${MG}?file=050.1 | tee >(md5sum > data/{}.md5sum)  | gzip > data/${MG}.fna.gz
done
```

This will zip the raw fasta files and save the md5sums to validate the download.

File list:
```
mgm4474351.3    mgm4474351.3.050.upload.fna	050.1	9ae1567e29c92695d4dba87368d48c91	2725914863
mgm4467109.3	mgm4467109.3.050.upload.fna	050.1	585207b8d1ed08054f64d83e2ea2c4b5	11994956359
mgm4466188.3	mgm4466188.3.050.upload.fna	050.1	06d134cc37353ae4ad61a114f864a680	16364852358
mgm4474355.3	mgm4474355.3.050.upload.fna	050.1	880b848b7b43042196121024ab84a226	2897033362
mgm4474361.3	mgm4474361.3.050.upload.fna	050.1	629cd1f2459d20ea17c2520191eb70d3	2216895877
mgm4466296.3	mgm4466296.3.050.upload.fna	050.1	835d93b4898cedba6fda521558f36b8c	17698558630
mgm4474805.3	mgm4474805.3.050.upload.fna	050.1	c2b677e9fdf510994a938ad40caeaaa1	2863660233
mgm4466602.3	mgm4466602.3.050.upload.fna	050.1	374d849ad2cf90d6cc0dd73f4406be41	17061324126
mgm4474369.3	mgm4474369.3.050.upload.fna	050.1	b6076f12d6e14386dcd907c7d743d06e	2856542403
mgm4474352.3	mgm4474352.3.050.upload.fna	050.1	b96c0fe18613e766ab05bd86aea11ae3	1846447145
mgm4466257.3	mgm4466257.3.050.upload.fna	050.1	b4106a991a7ed7b4ea2a74b5b4c7183b	12188538201
mgm4474360.3	mgm4474360.3.050.upload.fna	050.1	cf599631d45f0618d43232a57f616d7c	2845729091
mgm4466194.3	mgm4466194.3.050.upload.fna	050.1	d51674481f637a48bf5a430fd6c2b507	12063818770
mgm4474358.3	mgm4474358.3.050.upload.fna	050.1	01d8bcc929984938ffc595a04e442122	1538258029
mgm4474357.3	mgm4474357.3.050.upload.fna	050.1	a6202d3e5dda9539970260c3f516af29	2290285473
mgm4466148.3	mgm4466148.3.050.upload.fna	050.1	7dfdd296b29c086d03ae777cb37549a3	16747713282
mgm4474359.3	mgm4474359.3.050.upload.fna	050.1	a3c700b971b1e4a0aa1b7e712c10f21d	2395798868
mgm4469858.3	mgm4469858.3.050.upload.fna	050.1	391d1538dec85fec2f72d1d7f7e3c663	8594938232
mgm4466149.3	mgm4466149.3.050.upload.fna	050.1	8f7a42a3a8c42b86e0f1a35fbb8113bc	7808085470
mgm4466163.3	mgm4466163.3.050.upload.fna	050.1	19f8dec05c5bb33f1810e06de68f98a7	14472271099
```

### Digital normalization

The reads were normalized by median using khmer with k-mer length of 20, count of 20 and hash size of 1e10 (Brown et al., 2012). This excluded from assembly any read that had a median k-mer of length 20 that had previously been encountered at least 20 times. This data reduction step removed redundant reads unlikely to add any more information to the assembly. The excluded reads were saved for downstream analysis. The remaining reads were filtered by abundance removing any low abundance (<5) and unique k-mers that are unlikely to assemble into larger contigs.

```
#files
SEQS=$1
#paths
KHMER_SCRIPTS="/home/kiverson/khmer/scripts" #path to khmer/scripts

#parameters
K=20
C=20
X_PRAM='1e10'

python $KHMER_SCRIPTS/normalize-by-median.py -k $K -C $C -x $X_PRAM --savetable $SEQS.kh $SEQS
python $KHMER_SCRIPTS/filter-abund.py -V $SEQS.kh $SEQS.keep
rm $SEQS.kh
```

### De-replicate

Technical replicates were removed using fastx_collapser. Removed reads were retained for downstream analysis.

```
for i in *.keep.abundfilt;
do
	fastx_collapser -i $i -o $i.fastx
done

```

### Download screen passed reads

Use MG-RAST's de-replication and filtering pipeline instead.

```
cat metagenomes.list | parallel "curl http://api.metagenomics.anl.gov//download/{}?file=299.1 | gzip > {}.fna.gz"
```

### Assembly

The de-replicated reads were pooled and assembled using an iterative assembly. Reads were first assembled with Velvet using a k-mer length of 71 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes) and unused reads were retained. The unused reads were then assembled with k-mer lengths 61, 51 and 41 with the same parameters. The contigs from each of these assemblies were merged with minimus2 (part of AMOS; Sommer et al., 2007) using default parameters.  An average of 54.43% (range 24.23% to 71.78%) of all reads across 20 metagenomes mapped back to the assembled contigs.

The filtered reads were pooled, `cat *.keep.abundfilt > all.abundfilt`, and assembled using an iterative assembly.

```
mkdir k.49;
velveth k.49 49 -fasta -short all.abundfilt;
velvetg k.49 -exp_cov auto -unused_reads yes;
rm k.49/Sequences;
rm k.49/*Graph*

for k in {47..21..2};
do
	mkdir k.$k;
	velveth k.$k $k -fasta -short all.abundfilt;
	velvetg k.$k -exp_cov auto -unused_reads yes;
	rm k.$k/Sequences;
	rm k.$k/*Graph*
done;
```

If using job submission software like pbs it would be a good idea so submit each velvet call as a dependent job.

Combine all contigs, `cat k.*/Contigs.fa > all.contigs.fa`.

### Megahit assembly

For this dataset, megahit gave the best assembly.

```
megahit -m 45e9 -r all.abundfilt.fastx --cpu-only -l 100 -o all.abundfilt.fastx_megahit

```

### Map reads to contigs

To get coverage map reads to the assembled contigs using bowtie.

```
bowtie-build
bowtie
```

### Gene prediction

Genes were predicted from assembled contigs with MetaGeneAnnotator(MGA) (Noguchi et al., 2008). The predicted genes were separated into two groups: having both a start and stop codon (full length) or fragments with a length greater than 300 nucleotides, including the full length group.

```
mga final.contigs.fa -m > final.contigs.fa.genes
```

MGA prints out the coordinates of the genes which can then be extracted from the contigs.

```
python ~/scripts/getgenesbypos.py final.contigs.fa.genes final.contigs.fa final.contigs.fa.genes.faa final.contigs.fa.genes.fna
```



### BLAST

Predicted genes equal to or longer than 100 amino acids were aligned to each other using the BLAST algorithm.

```
formatdb -i preg.megahitgenes100.faa -p T -n preg.megahitgenes100.blastDB
blastp -query preg.megahitgenes100.faa -db preg.megahitgenes100.blastDB -out allvall.megahitgenes100.blast.out -evalue 1e-5 -outfmt 6 -max_target_seqs 10000 -num_threads 16
```

### Get gene counts

Gene counts were determined by normalizing the number of reads mapped to each gene. As with the contigs, read mapping was done with bowtie.

Build bowtie database:

```
bowtie2-build final.contigs.fa.genes.fna preg.genes.fna.bowtieDB
```

```
bowtie2-build final.contigs.fa.genes.fna preg.genes.fna.bowtieDB
bowtie2 preg.genes.fna.bowtieDB -f ../all.fna -p 16 -S aligned2genes.sam
samtools view -bS aligned2genes.sam > aligned2genes.bam
rm aligned2genes.sam
samtools sort aligned2genes.bam aligned2genes.sorted
samtools index aligned2genes.sorted.bam
samtools idxstats aligned2genes.sorted.bam > mapped2genes.txt
```

Gene counts by group (this must be done in bash in order for the process substitution to work):

```
#!/bin/bash
set -e
set -o pipefail
READS=$1
BASE=$(basename $READS .fna.gz)
bowtie2 preg.genes.fna.bowtieDB -f <(zcat $READS) -p 16 -S $BASE.aligned2genes.sam

samtools view -bS $BASE.aligned2genes.sam > $BASE.aligned2genes.bam
rm $BASE.aligned2genes.sam
samtools sort $BASE.aligned2genes.bam $BASE.aligned2genes.sorted
rm $BASE.aligned2genes.bam

samtools index $BASE.aligned2genes.sorted.bam
samtools idxstats $BASE.aligned2genes.sorted.bam > $BASE.mapped2genes.txt
```

### Make count file

Count files are files used by mothur to indicate counts of representative genes without having to load all of the sequences into memory.

For all groups combined:

```
makecountfilefrombowtie.py -f final.contigs.fa.genes.faa -o preg.genes.counts -l mapped2genes.txt
```

Mapping by group:

```
makecountfilefrombowtie.py -f final.contigs.fa.genes.faa -o preg.genes.counts -l mgm*.mapped2genes.txt
```


### Create OPFs

Use the average neighbor clustering algorithm in mg-cluster to create OPF clusters.

```
mothur > mgcluster()
```

### Get distances

```
dist.shared(shared=allVall100.an.unique_list.shared, processors=16, calc=thetayc, subsample=2515565)
mothur > dist()
```

### R stuff

Read in lower triangle distance matrix

```
matrixcols <- readLines("preg.eggnog.thetayc.1.lt.dist", n=1)
d <- read.table(file="preg.eggnog_nona.thetayc.1.lt.dist", fill=TRUE, header=F, skip=1, row.names=1, col.names=paste("V", 1:matrixcols))
d$x<-NA
mat<-data.matrix(d)
mat[is.na(mat)]<-0
mat <- mat + t(mat)
colnames(mat)<-rownames(mat)
```


## Lean and obese twin dataset

### Download data

The data is available through the NCBI sequence read archive (SRA) under study [SRP000319](http://trace.ncbi.nlm.nih.gov/Traces/sra/?study=SRP000319). The metagenome accession numbers are:

metagenomes.txt:

```
SRR029686
SRR029703
SRR029702
SRR029701
SRR029700
SRR029699
SRR029698
SRR029697
SRR029696
SRR029695
SRR029694
SRR029693
SRR029692
SRR029691
SRR029690
SRR029689
SRR029688
SRR029687
```

These metagenomes can also be referenced by library name.

```
SRR029686 TS19
SRR029687 TS1
SRR029688 TS20
SRR029689 TS21
SRR029690 TS28
SRR029691 TS29
SRR029692 TS2
SRR029693 TS30
SRR029694 TS3
SRR029695 TS49
SRR029696 TS4
SRR029697 TS50
SRR029698 TS51
SRR029699 TS5
SRR029700 TS6
SRR029701 TS7
SRR029702 TS8
SRR029703 TS9
```

Use the [SRA toolkit](http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=software) to download the fastq files for this study.

```
for i in `cat metagenomes.txt`;
do
	fastq-dump --gzip $i;
done

```

### Assembly

The reads from the twin study were sequenced with 454 pyrosequencing and are longer than the Illumina reads used in the pregnancy study, therefore the long option is required for velvet.

```
velveth assembly 51 -fastq -long *.fastq
velvetg assembly -exp_cov auto -min_contig_lgth 100
```

### Alignment

Align the reads to the contigs to asess coverage. These are 454 reads so an aligner that can handle long reads, like bwa-sw, must be used.

```
bwa index -a bwtsw twinSRAgenes300.fna
mv twinSRAgenes300.fna.* bwa/
cd reads
bwasw ref reads > aln.sam

bwa-sw reads contigs.fa
```

### Gene prediction

Genes were predicted from assembled contigs with MetaGeneAnnotator (Noguchi et al., 2008). The predicted genes were separated into two groups: having both a start and stop codon (full length) or fragments with a length greater than 300 nucleotides, including the full length group.

```
mga
mga allmerged.renamed.fa -m > genes.mga
```

### BLAST

Predicted genes equal to or longer than 100 amino acids were aligned to each other using the BLAST algorithm.

```
blastp -query genes100.faa -db genes100.DB -out allvall.genes100.blast.out -evalue 1e-5 -outfmt 6 -max_target_seqs 10000 -num_threads 8
```

### Get gene counts

Gene counts were determined by normalizing the number of reads mapped to each gene. As with the contigs, read mapping was done with bowtie.

```
bowtie2-build final.contigs.fa.genes.fna twin.genes.fna.bowtieDB
bowtie2 twin.genes.fna.bowtieDB -f ../all.fna -p 16 -S aligned2genes.sam
```

### Make count file

Count files are files used by mothur to indicate counts of representative genes without having to load all of the sequences into memory.

```
makecountfilefrombowtie.py
```

### Create OPFs

Use the average neighbor clustering algorithm in mg-cluster to create OPF clusters.

```
mothur > mgcluster()
```
mgcluster -> list file
make.shared -> shared file
sub.sample
dist.shared -> dist file
nmds -> plot


HMP
de-replicate at 99% BEFORE blast to reduce data size
usearch for all v all
ALL READS mapping for counts


Gene prediction
Genes were predicted from assembled contigs with MetaGeneAnnotator (Noguchi et al., 2008). The predicted genes were separated into two groups: having both a start and stop codon (full length) or fragments with a length greater than 300 nucleotides, including the full length group.

Functional and taxonomic classification
Predicted gene fragments were searched against the eggNOG database (Muller et al., 2010) by BLASTp. Significant hits that had e-values > 10^-5 were grouped into eggNOG categories. Significant hits were defined as any hit that had an e-value <= 10*e-value of the top hit (Qin et al., 2010).

OPF creation and data analysis
Abundance was calculated by number of reads mapping onto predicted genes and fragments,  and normalized for gene length. Mapping was done using bowtie (Langmead et al., 2009). OPFs were created using the mgcluster command in mothur (Schloss et al. 2009). For all analyses the datasets were subsampled to the lowest number of sequences per sample. A Principal component analysis  (PCoA) plot was generated in mothur using the average Yue-Clayton measure of dissimilarity distances (θYC; REF). Plots were generated with R (R Development Core Team, 2012). Similarly a distance matrix was generated from abundance of KEGG categories.
The OPF abundances were transformed into binary data and OPF occurrences within and between groups of samples inside each dataset were analyzed with R.  For the mice data, we combined the 16S and OPF data using correlation analysis and these results are summarized in a heatmap showing the 20 OTUs that had significantly different abundance during the early and late periods and the OPFs correlating with these.


## HMP Keratinized gingiva

### Get data

Download called genes from HMP

```
wget ftp://public-ftp.hmpdacc.org/HMGI/attached_keratinized_gingiva/*
```

File list:

```
SRS013946.with_fasta.gff3.bz2   SRS014473.with_fasta.gff3.bz2   SRS014687.with_fasta.gff3.bz2   SRS015060.with_fasta.gff3.bz2   SRS019025.with_fasta.gff3.bz2   SRS019125.with_fasta.gff3.bz2
SRS013946_aa.fasta.bz2          SRS014473_aa.fasta.bz2          SRS014687_aa.fasta.bz2          SRS015060_aa.fasta.bz2          SRS019025_aa.fasta.bz2          SRS019125_aa.fasta.bz2
SRS013946_nucleotide.fasta.bz2  SRS014473_nucleotide.fasta.bz2  SRS014687_nucleotide.fasta.bz2  SRS015060_nucleotide.fasta.bz2  SRS019025_nucleotide.fasta.bz2  SRS019125_nucleotide.fasta.bz2
```

Download raw reads (for counts)

```
wget ftp://public-ftp.hmpdacc.org/Illumina/attached_keratinized_gingiva/*
```

File list:

```
2f3eb882d63b22a16352a6107957178f  SRS013946.tar.bz2
47dcb8d6485d9a7b5a1998b1c2270977  SRS014473.tar.bz2
e7db62f393c32197d5f4784fb3b19d76  SRS014687.tar.bz2
7633c105e0bf63fadeec5c02ef9076f3  SRS015060.tar.bz2
78c202438e8729197f6b1d76981f231d  SRS019025.tar.bz2
b3bfd385662f10c9c87187233c801dd7  SRS019125.tar.bz2
```

### BLAST

All v all blast

```
formatdb -i all.aa.genes100.fasta -p T -n all.aa.genes100.fasta.blastDB
blastp -query all.aa.genes100.fasta -db all.aa.genes100.fasta.blastDB -out allvall.aa.genes100.out -evalue 1e-5 -outfmt 6 -max_target_seqs 10000 -num_threads 8
```


### Map reads to genes

Build a database

```
bowtie-build all.nucleotide.genes300.fasta all.nucleotide.genes300.fasta.bowtieDB
```

map reads

```
#!/bin/bash
set -e
set -o pipefail
READS=$1
BASE=$(basename $READS .tar.bz2)

bzcat $READS | tar -xvf -

bowtie ../calledgenes/all.nucleotide.genes300.fasta.bowtieDB -1 $BASE/$BASE.denovo_duplicates_marked.trimmed.1.fastq -2 $BASE/$BASE.denovo_duplicates_marked.trimmed.2.fastq -p 16 -S $BASE.pe.aligned2genes.sam

samtools view -bS $BASE.pe.aligned2genes.sam > $BASE.pe.aligned2genes.bam
rm $BASE.pe.aligned2genes.sam
samtools sort $BASE.pe.aligned2genes.bam $BASE.pe.aligned2genes.sorted
rm $BASE.pe.aligned2genes.bam

samtools index $BASE.pe.aligned2genes.sorted.bam
samtools idxstats $BASE.pe.aligned2genes.sorted.bam > $BASE.pe.mapped2genes.txt


bowtie ../calledgenes/all.nucleotide.genes300.fasta.bowtieDB -q $BASE/$BASE.denovo_duplicates_marked.trimmed.singleton.fastq -p 16 -S $BASE.se.aligned2genes.sam

samtools view -bS $BASE.se.aligned2genes.sam > $BASE.se.aligned2genes.bam
rm $BASE.se.aligned2genes.sam
samtools sort $BASE.se.aligned2genes.bam $BASE.se.aligned2genes.sorted
rm $BASE.se.aligned2genes.bam

samtools index $BASE.se.aligned2genes.sorted.bam
samtools idxstats $BASE.se.aligned2genes.sorted.bam > $BASE.se.mapped2genes.txt
```

Add counts for genes that didn't have reads map to them (count as 0)

```
getMissingGenes.py kg.genes.count ../calledgenes/allvall.aa.genes100.out missingGenes.txt && cat kg.genes.count missingGenes.txt > kg.genes.ALL.count
```

### Cluster

Cluster genes with mgcluster
```
mothur '#mgcluster(blast=/mnt/EXT/Schloss-data/kiverson/hmp_kg/calledgenes/allvall.aa.genes100.out, count=/mnt/EXT/Schloss-data/kiverson/hmp_kg/cluster/kg.genes.ALL.count
```


## HMP Saliva

### Get data

Download raw reads

```
wget ftp://public-ftp.hmpdacc.org/Illumina/saliva/*
```

Files:

```
11cb282fd2bd3235963a0ec404385aee  SRS013942.tar.bz2
04e6659bf16ab6346533110a17c6324f  SRS014468.tar.bz2
7d9e128a12bfd8e3aca2c4fa534aed70  SRS014692.tar.bz2
2841467ebbbff274c20d7caa931faaa2  SRS015055.tar.bz2
c884c9135361f2ae41db336df63f5e09  SRS019120.tar.bz2
```

Download genes

```
wget ftp://public-ftp.hmpdacc.org/HMGI/saliva/*
```

Files:

```
55f2d8d146f32e7f89191fd3061daa66  SRS013942_aa.fasta.bz2
0cdcec08b194433ccd06887d4d8707e5  SRS013942_nucleotide.fasta.bz2
e71f8c8691dd97c046711a979cfd1e12  SRS013942.with_fasta.gff3.bz2
e2039cf4fe9f557be4fd96aa805a9853  SRS015055_aa.fasta.bz2
5f14cb207ccc1b70411ffa56aac483e9  SRS015055_nucleotide.fasta.bz2
d3aa53bab98e0ee635f0365f250a7c2c  SRS015055.with_fasta.gff3.bz2
7d1ff4e4f8e4f3d9b2e0b7f95cbd92d7  SRS019120_aa.fasta.bz2
9a83e6da8c43c9c10216c3839c924a6e  SRS019120_nucleotide.fasta.bz2
4f97d628d591988e9c05a00c418995fe  SRS019120.with_fasta.gff3.bz2
```

Files listed in ftp://public-ftp.hmpdacc.org/HMGI/md5sums.txt

```
e71f8c8691dd97c046711a979cfd1e12  saliva/SRS013942.with_fasta.gff3.bz2
d3aa53bab98e0ee635f0365f250a7c2c  saliva/SRS015055.with_fasta.gff3.bz2
4f97d628d591988e9c05a00c418995fe  saliva/SRS019120.with_fasta.gff3.bz2
```

### BLAST

Get genes > 100 amino acids

```
cat *.fasta > all.genes.aa.fasta
python ~/scripts/removeShortContigs.py all.genes.aa.fasta all.aa.genes100.fasta 100
```

All v all BLAST

```
formatdb -i all.aa.genes100.fasta -p T -n all.aa.genes100.fasta.blastDB
blastp -query all.aa.genes100.fasta -db all.aa.genes100.fasta.blastDB -out allvall.aa.genes100.out -evalue 1e-5 -outfmt 6 -max_target_seqs 10000 -num_threads 8
```


### Map reads to genes

Build a database

```
cat *_nucleotide.fasta > all.nucleotide.fasta
python ~/scripts/removeShortContigs.py all.nucleotide.fasta all.nucleotide.genes300.fasta 300
bowtie-build all.nucleotide.genes300.fasta all.nucleotide.genes300.fasta.bowtieDB
```

map reads

```
#!/bin/bash
set -e
set -o pipefail
READS=$1
BASE=$(basename $READS .tar.bz2)

bzcat $READS | tar -xvf -

bowtie ../genes/all.nucleotide.genes300.fasta.bowtieDB -1 $BASE/$BASE.denovo_duplicates_marked.trimmed.1.fastq -2 $BASE/$BASE.denovo_duplicates_marked.trimmed.2.fastq -p 16 -S $BASE.pe.aligned2genes.sam

samtools view -bS $BASE.pe.aligned2genes.sam > $BASE.pe.aligned2genes.bam
rm $BASE.pe.aligned2genes.sam
samtools sort $BASE.pe.aligned2genes.bam $BASE.pe.aligned2genes.sorted
rm $BASE.pe.aligned2genes.bam

samtools index $BASE.pe.aligned2genes.sorted.bam
samtools idxstats $BASE.pe.aligned2genes.sorted.bam > $BASE.pe.mapped2genes.txt
rm $BASE.pe.aligned2genes.sorted.bam


bowtie ../genes/all.nucleotide.genes300.fasta.bowtieDB -q $BASE/$BASE.denovo_duplicates_marked.trimmed.singleton.fastq -p 16 -S $BASE.se.aligned2genes.sam

samtools view -bS $BASE.se.aligned2genes.sam > $BASE.se.aligned2genes.bam
rm $BASE.se.aligned2genes.sam
samtools sort $BASE.se.aligned2genes.bam $BASE.se.aligned2genes.sorted
rm $BASE.se.aligned2genes.bam

samtools index $BASE.se.aligned2genes.sorted.bam
samtools idxstats $BASE.se.aligned2genes.sorted.bam > $BASE.se.mapped2genes.txt
rm $BASE.se.aligned2genes.sorted.bam

rm -rf $BASE
```

### Make count file

```
python ../../opf/code/makecountfilefrombowtie.py -f ../genes/all.aa.genes100.fasta -o saliva.genes.count -l *.txt
```

Add counts for genes that didn't have reads map to them (count as 0)

```
getMissingGenes.py kg.genes.count ../calledgenes/allvall.aa.genes100.out missingGenes.txt && cat kg.genes.count missingGenes.txt > kg.genes.ALL.count
```

### Cluster

Cluster genes with mgcluster
```
mothur '#mgcluster(blast=/mnt/EXT/Schloss-data/kiverson/hmp_saliva/genes/allvall.aa.genes100.out, count=saliva.ALL.genes.count)'
mothur '#make.shared(count=saliva.ALL.genes.count, list=allvall.aa.genes100.an.unique_list.list)'
```

## HMP left retroauricular crease

### Get data

Get reads:

```
wget ftp://public-ftp.hmpdacc.org/Illumina/left_retroauricular_crease/*
```

reads, md5sum .bz2:

```
a56827ec36c7f7273f7219d4d54a196a  SRS013258.tar.bz2
225af4d878d9a3c8d2a0f2a20be4243a  SRS016944.tar.bz2
50452b9b04db83d1e39014e46cf3838b  SRS017849.tar.bz2
a3c1348de933d8b9c017d416d48be2ea  SRS019015.tar.bz2
de7fb60a1680c08aba666f1cfe6d5f39  SRS019063.tar.bz2
e79a5e2b0d3e82dc3db97b346e7b4c9a  SRS020261.tar.bz2
cd4ee71a7af859dba4dfe43bbd573e2c  SRS024482.tar.bz2
8bac9dd877bf1480676c984f79558168  SRS024596.tar.bz2
a286cdc62000286673efaf67b5c72266  SRS024620.tar.bz2
```

Genes:

```
wget ftp://public-ftp.hmpdacc.org/HMGI/left_retroauricular_crease/*
```

```
482f432515c673264f85382e9283676b  SRS013258_aa.fasta.bz2
1ecfed9aa54796d453f26c9399ec64b2  SRS013258_nucleotide.fasta.bz2
609a49dc1eebd1452a6b960b95488c07  SRS013258.with_fasta.gff3.bz2
36b24778cf88361df3edcdd73957f8b7  SRS016944_aa.fasta.bz2
7c7a6f48826f0bc5ad5c20d9a732614c  SRS016944_nucleotide.fasta.bz2
6ae331b977f65cf8c66a9aa689cf410d  SRS016944.with_fasta.gff3.bz2
1e34d6fece02e14ea1f02518b0a74647  SRS017849_aa.fasta.bz2
f58a27d36ab3c41b14aece615f4e8e46  SRS017849_nucleotide.fasta.bz2
c22cb8e1076b8c3d5e41eb389f5dcc65  SRS017849.with_fasta.gff3.bz2
99de47244373d6f04d4cdb5a9b821050  SRS019015_aa.fasta.bz2
0af6d599800a78bf70a877ae6a17b31b  SRS019015_nucleotide.fasta.bz2
3248d758db5ff8258591d46b3c75d490  SRS019015.with_fasta.gff3.bz2
8f65326ed31c73223360fadfb7cd2b95  SRS019063_aa.fasta.bz2
7db24ac2ea8dac576bebb5b553d272da  SRS019063_nucleotide.fasta.bz2
cacb2cf8d2bb9df2d5007f816e1fa49b  SRS019063.with_fasta.gff3.bz2
c7273eff5cf0858bdb979379f9265973  SRS020261_aa.fasta.bz2
add8ad4d6c8e7575c70d118bf39b73ec  SRS020261_nucleotide.fasta.bz2
aad412fca16392d432a9fd9522bd1a00  SRS020261.with_fasta.gff3.bz2
b092b0ce63051fe8078a800353b15ce7  SRS024482_aa.fasta.bz2
b825535dfa943421d34e44797180dd10  SRS024482_nucleotide.fasta.bz2
11513918ca19ab0902cf9d7bff4d49b6  SRS024482.with_fasta.gff3.bz2
edfb276d56666ddc29b68dab4c7186b1  SRS024596_aa.fasta.bz2
e0d1e606dcb9405d956e3d2315939732  SRS024596_nucleotide.fasta.bz2
7d3e7936f653b79cc7cea057688a3abc  SRS024596.with_fasta.gff3.bz2
ec778e17c6143720aa568238b84428dd  SRS024620_aa.fasta.bz2
7bb9eabaf860237ec73e1a4751eadc7c  SRS024620_nucleotide.fasta.bz2
29c31244ca8d6bb26d9a7e099b608057  SRS024620.with_fasta.gff3.bz2
```

md5sums from ftp://public-ftp.hmpdacc.org/HMGI/md5sums.txt

```
609a49dc1eebd1452a6b960b95488c07  left_retroauricular_crease/SRS013258.with_fasta.gff3.bz2
6ae331b977f65cf8c66a9aa689cf410d  left_retroauricular_crease/SRS016944.with_fasta.gff3.bz2
c22cb8e1076b8c3d5e41eb389f5dcc65  left_retroauricular_crease/SRS017849.with_fasta.gff3.bz2
3248d758db5ff8258591d46b3c75d490  left_retroauricular_crease/SRS019015.with_fasta.gff3.bz2
cacb2cf8d2bb9df2d5007f816e1fa49b  left_retroauricular_crease/SRS019063.with_fasta.gff3.bz2
aad412fca16392d432a9fd9522bd1a00  left_retroauricular_crease/SRS020261.with_fasta.gff3.bz2
11513918ca19ab0902cf9d7bff4d49b6  left_retroauricular_crease/SRS024482.with_fasta.gff3.bz2
7d3e7936f653b79cc7cea057688a3abc  left_retroauricular_crease/SRS024596.with_fasta.gff3.bz2
29c31244ca8d6bb26d9a7e099b608057  left_retroauricular_crease/SRS024620.with_fasta.gff3.bz2
```

### All v all BLAST

```
bzcat *_aa.fasta.bz2 > all.aa.genes.fasta && python ~/scripts/removeShortContigs.py all.aa.genes.fasta all.aa.genes100.fasta 100
formatdb -i all.aa.genes100.fasta -p T -n all.aa.genes100.fasta.blastDB
blastp -query all.aa.genes100.fasta -db all.aa.genes100.fasta.blastDB -out allvall.aa.genes100.out -evalue 1e-5 -outfmt 6 -max_target_seqs 10000 -num_threads 8
```

### Map reads to genes

Make bowtie database

```
bzcat *_nucleotide.fasta.bz2 > all.nucleotide.genes.fasta && python ~/scripts/removeShortContigs.py all.nucleotide.genes.fasta all.nucleotide.genes300.fasta 300

bowtie-build all.nucleotide.genes300.fasta all.nucleotide.genes300.fasta.bowtieDB
```

Bowtie to map reads

```
#!/bin/bash
set -e
set -o pipefail
READS=$1
BASE=$(basename $READS .tar.bz2)

bzcat $READS | tar -xvf -

bowtie ../genes/all.nucleotide.genes300.fasta.bowtieDB -1 $BASE/$BASE.denovo_duplicates_marked.trimmed.1.fastq -2 $BASE/$BASE.denovo_duplicates_marked.trimmed.2.fastq -p 16 -S $BASE.pe.aligned2genes.sam

samtools view -bS $BASE.pe.aligned2genes.sam > $BASE.pe.aligned2genes.bam
rm $BASE.pe.aligned2genes.sam
samtools sort $BASE.pe.aligned2genes.bam $BASE.pe.aligned2genes.sorted
rm $BASE.pe.aligned2genes.bam

samtools index $BASE.pe.aligned2genes.sorted.bam
samtools idxstats $BASE.pe.aligned2genes.sorted.bam > $BASE.pe.mapped2genes.txt
rm $BASE.pe.aligned2genes.sorted.bam


bowtie ../genes/all.nucleotide.genes300.fasta.bowtieDB -q $BASE/$BASE.denovo_duplicates_marked.trimmed.singleton.fastq -p 16 -S $BASE.se.aligned2genes.sam

samtools view -bS $BASE.se.aligned2genes.sam > $BASE.se.aligned2genes.bam
rm $BASE.se.aligned2genes.sam
samtools sort $BASE.se.aligned2genes.bam $BASE.se.aligned2genes.sorted
rm $BASE.se.aligned2genes.bam

samtools index $BASE.se.aligned2genes.sorted.bam
samtools idxstats $BASE.se.aligned2genes.sorted.bam > $BASE.se.mapped2genes.txt
rm $BASE.se.aligned2genes.sorted.bam

rm -rf $BASE
```


## Posterior fornix

## Map reads to genes

Nonconforming datasets:

```
SRS019245
SRS078182
```

Removed records:

```

```

bowtie -- multiple single end reads

```
#!/bin/bash
set -e
set -o pipefail
READS=$1
BASE=$(basename $READS .tar.bz2)

cat $BASE/*.fastq > all.fastq

bowtie ../genes/all.nucleotide.genes300.fasta.bowtieDB -q $BASE/all.fastq -p 16 -S $BASE.se.aligned2genes.sam

samtools view -bS $BASE.se.aligned2genes.sam > $BASE.se.aligned2genes.bam
rm $BASE.se.aligned2genes.sam
samtools sort $BASE.se.aligned2genes.bam $BASE.se.aligned2genes.sorted
rm $BASE.se.aligned2genes.bam

samtools index $BASE.se.aligned2genes.sorted.bam
samtools idxstats $BASE.se.aligned2genes.sorted.bam > $BASE.se.mapped2genes.txt
rm $BASE.se.aligned2genes.sorted.bam

rm -rf $BASE
```

## Tongue dorsum

Break up fasta file -- too long for bowtie-build

```
python ~/scripts/breakupfasta.py all.nucleotide.genes300.fasta 5000000
```

KEGG BLAST using usearch

```
/mnt/EXT/Schloss-data/usearch6.0.307_i86linux64 -ublast all.aa.genes100.fasta -db /mnt/EXT/Schloss-data/dbs/KEGGdb/genes.pep.udb -evalue 1e-5 -blast6out all.aa.genes100vKEGG.b6
```


```
cmd = "/mnt/EXT/Schloss-data/usearch6.0.307_i86linux64 -ublast {0}-gene-seqs.faa -db /mnt/EXT/Schloss-data/dbs/KEGGdb/genes.pep.udb -evalue 1e-5 -blast6out {0}vKEGG.b6".format(base[0])
```

## Buccal muscosa

### remove bad reads

Dataset SRS043422, removed reads:

```
awk '{if (length  >1024) {print NR} }' SRS043422.denovo_duplicates_marked.trimmed.2.fastq
37907293
37908751
awk '{if (length  >1024) {print NR} }' SRS043422.denovo_duplicates_marked.trimmed.1.fastq
37919045
awk '{if (length  >1024) {print NR} }' SRS043422.denovo_duplicates_marked.trimmed.singleton.fastq
6290160
```


=======================

Download the data from the twin study the same way. The only difference is this is MG-RAST project 10 (mgp10)

```
cd ../twin_data
../MG-RAST-Tools/tools/bin/mg-download.py --project mgp10 --list | sed 's/\t.*$//g' | sed '1d' | uniq > metagenomes.list
cat metagenomes.list | parallel "curl http://api.metagenomics.anl.gov//download/{}?file=050.1 | tee >(md5sum > {}.md5sum) | gzip > {}.fna.gz"
```


For the mice study the primer sequences were removed from raw reads using SeqPrep (available at https://github.com/jstjohn/seqPrep).

```
seqPrep
```



```
khmer
```

Obesity study dataset.  

The sequence reads from the 18 metagenomes generated by Turnbaugh and colleagues (2009) in their study of the microbiome of lean and obese individuals were obtained from MG-RAST.

```
wget
```

The reads from all metagenomes were pooled and assembled with Velvet (Zerbino and Birney, 2008) using k 51 (parameters -exp_cov auto -min_contig_lgth 100). The reads from this study were treated as ‘long’ because they were sequenced with 454 pyrosequencing (mean length 216bp, range 9bp to 665bp) whereas the Illumina reads from the pregnancy and mouse studies were treated as ‘short’ (mean length 99bp and 98bp, respectively).  An average of 22.13% (range 4.72% to 35.90%) of all reads across 18 metagenomes mapped back to the assembled contigs.

Pregnancy study dataset.  

The sequence reads from the 20 metagenomes generated by Koren and colleageus (Koren et al., 2012) in their study of the effects of pregnancy on the mother’s microbiome were obtained from MG-RAST.  The reads were pooled and assembled using an iterative assembly. Reads were first assembled with Velvet using a k-mer length of 71 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes) and unused reads were retained. The unused reads were then assembled with k-mer lengths 61, 51 and 41 with the same parameters. The contigs from each of these assemblies were merged with minimus2 (part of AMOS; Sommer et al., 2007) using default parameters.  An average of 54.43% (range 24.23% to 71.78%) of all reads across 20 metagenomes mapped back to the assembled contigs.

Mouse gut microbiome stability dataset.  

Using the Illumina HiSeq and standard methods (http://hmpdacc.org/doc/HMP_MOP_Version12_0_072910.pdf), we generated 48 metagenomes from samples for which we previously obtained 16S rRNA gene sequence data (Schloss et al., 2012).  The sequencing was performed at the Baylor College of Medicine (Houston, TX).  All reads from the mice study were pooled into two groups corresponding to early and late samples. After normalization 15% of the late reads (false positive rate of 0.000) and 17% of the early (false positive rate of 0.258) were retained for assembly. The unused reads were set aside and used in downstream analysis.  The retained reads were assembled twice with Velvet using k 31 and k 35 (parameters -exp_cov auto -cov_cutoff 0 -scaffolding no). Both assemblies were merged with minimus2 using default parameters.  An average of 84% (range XX.XX% to XX.XX%) of all reads across 48 metagenomes mapped back to the assembled contigs.

Gene prediction and classification.  

Genes were predicted from assembled contigs with MetaGeneAnnotator (Noguchi et al., 2008). The predicted genes were filtered to only include predicted genes and fragments with a length greater than 300 nucleotides.  Predicted genes and fragments were translated and searched against the KEGG database (Kanehisa et al., 2012) by BLASTp. Significant hits that had e-values less than 10^-5 were grouped into KEGG modules.  Significant hits were defined as any hit that had an e-value less than or equal to 10 times the e-value of the top hit (Qin et al., 2010).

OPF creation and data analysis.  

Abundance was calculated by number of reads mapping onto predicted gene fragments and normalized for gene length. Mapping was done using Bowtie (Langmead et al., 2009) for the mice and pregnancy reads and BWA-SW (Li and Durbin, 2010) for the obesity reads.  OPFs were created from the predicted genes and fragments using the mgcluster command in mothur (Schloss et al., 2009).  To minimize the effects of uneven sampling, within a dataset each sample was rarefied to the lowest number of sequences per sample.  Principal component analysis (PCoA) plots were generated in mothur using the average Yue-Clayton measure of dissimilarity distances (θYC; Yue and Clayton, 2005) or weighted or unweighted Unifrac distances (Lozupone and Knight, 2005).  Statistical testing was done within the R environment (R Development Core Team, 2012) and mothur.




Data processing
Quality assurance, normalization
SeqPrep (available at https://github.com/jstjohn/seqPrep) was used to remove primer sequences from reads.  Reads were normalized by median using khmer with k-mer length of 20, count of 20, hash size 1e9 and 4 hashes (parameters -k 20 -C 20 -x 1e9 -N 4) (Brown et al. 2012). This excluded from assembly any read that had a median k-mer of length 20 that had previously been encountered at least 20 times. This data reduction step removed redundant reads unlikely to add any more information to the assembly. The excluded reads were saved for downstream analysis. The remaining reads were filtered by abundance using filter-abund.py. This removed any low abundance and unique k-mers that are unlikely to assemble into larger contigs.

Assembly
To demonstrate robustness to assembly methods the normalized reads were assembled using two assembly methods.

Mice and twin
Reads were assembled twice with velvet (Zerbino and Birney, 2008) using k 31 and k 35 (parameters -exp_cov auto -cov_cutoff 0 -scaffolding no). Both assemblies were merged with minimus2,part of AMOS (Sommer et al., 2007), using default parameters.

Pregnancy
Reads from the pregnancy data were assembled using an iterative assembly. Reads were assembled with velvet using k 71 and unused reads were retained. These unused reads were then assembled with k 61, k 51 and k 41 (parameters -exp_cov auto -read_trkg yes -amos_file yes -min_contig_lgth 200 -unused_reads yes). The contigs from each of these assemblies were merged with minimus2 using default parameters.
